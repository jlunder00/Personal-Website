---
title: "Tree Matching Networks: What If We Gave Neural Networks the Parse Tree Instead?"
date: 2026-02-08
description: "How encoding sentences as dependency trees and matching them with graph neural networks can outperform BERT baselines on natural language inference -- with a fraction of the parameters."
tags: ["NLP", "deep learning", "graph neural networks", "research"]
show_reading_time: true
draft: true
---

When we feed text to a transformer, the model has to figure out the structure of language entirely from data. Subject-verb relationships, modifier scope, negation boundaries -- all of it must be learned implicitly from millions (or billions) of examples. But what if we just... told the model the structure upfront?

That's the core idea behind [Tree Matching Networks](https://arxiv.org/abs/2512.00204) (TMN), my recent research exploring whether dependency parse trees can serve as an efficient structural prior for natural language inference. The short version: a graph neural network operating on parse trees significantly outperformed a BERT baseline of comparable size on the SNLI benchmark -- while being trained on a single GPU.

This post walks through the motivation, architecture, and what I think it means for the field.

## The question behind the question

Transformers work. That's not in dispute. But *why* they work -- and specifically, *what* structural information they're learning internally -- is still an active area of research. Attention patterns in BERT have been shown to loosely correspond to dependency relations ([Clark et al., 2019](https://aclanthology.org/W19-4828/)), suggesting the model spends some of its capacity rediscovering linguistic structure that we already know how to extract.

This raises a practical question: if we hand the model that structure explicitly, can we get comparable performance with less compute? Not as a replacement for transformers, but as a way to understand what they're actually doing -- and whether there's a more efficient path.

## From tokens to trees

A standard NLP pipeline treats a sentence as a sequence of tokens. A dependency parser instead produces a tree, where each word is a node and edges represent grammatical relationships.

Take the sentence *"The cat sat on the mat."* As a token sequence, the model sees six tokens in order. As a dependency tree, it sees that *sat* is the root, *cat* is its subject (nsubj), *mat* is the object of the preposition, and *the* modifies both nouns. The hierarchical relationships are explicit.

```
        sat (ROOT)
       /    \
    cat      on
    |        |
   The      mat
             |
            the
```

This representation isn't new -- dependency parsing has been a core NLP tool for decades. What's new is using these trees as the *input representation* for a neural network designed to compare sentence pairs, rather than using them as a feature extraction step or auxiliary training signal.

## Architecture: adapting graph matching for language

TMN builds on [Graph Matching Networks](https://arxiv.org/abs/1904.12787) (GMN), originally designed for comparing arbitrary graph structures. The key adaptation is replacing generic graph inputs with dependency parse trees generated by [DiaParser](https://github.com/Elmo-Lin/diaparser) and enriched with word embeddings from SpaCy.

The architecture has three main components:

**1. Node and edge encoding.** Each word in the parse tree becomes a node with an initial embedding. Dependency relation labels (nsubj, dobj, amod, etc.) become edge features. This gives the network both semantic (what the word means) and syntactic (how it relates to other words) information from the start.

**2. Message-passing propagation.** Nodes iteratively update their representations by aggregating information from their neighbors in the tree. After several rounds of propagation, each node's representation encodes not just its own meaning, but its structural context -- a verb "knows" about its subject and object, a modifier "knows" what it modifies. This is the core GNN mechanism: local structure gets encoded into node states through repeated message passing.

**3. Cross-graph matching.** For comparing two sentences (as required for natural language inference), TMN computes attention-weighted correspondences between nodes across the two trees. This cross-graph attention allows the model to align semantically similar subtrees between the premise and hypothesis, then aggregate these alignments into a final similarity score.

## Training: making it work on one GPU

One of the practical constraints of this research was compute: a single NVIDIA RTX 3090 on a desktop machine. No cluster. No cloud budget. This shaped the training strategy significantly.

**Multi-stage contrastive learning.** Rather than jumping straight to 3-class NLI classification (entailment, neutral, contradiction), TMN uses a staged approach:

1. **Pretraining with InfoNCE contrastive loss** on sentence pairs from WikiQS and AmazonQA (~7M sentences). This teaches the model basic semantic similarity -- push similar sentences together, dissimilar ones apart.

2. **Primary training with multi-objective InfoNCE** on SNLI's 550K labeled pairs. Instead of hard classification, this stage uses a softer objective: entailed pairs should be more similar than neutral pairs, which should be more similar than contradictory pairs. The relative ordering matters more than absolute boundaries.

This staged approach creates a smoother learning curve that works within the constraints of limited compute. With larger models and more GPUs, you can often get away with training everything end-to-end. With 280 GPU-hours on a single card, curriculum matters.

**Why InfoNCE specifically?** It's well-understood for embedding tasks, works naturally with the similarity-based framing of NLI, and extends cleanly to the multi-class setting. The multi-objective variant lets us express the three-way relationship (entailment > neutral > contradiction in similarity) without forcing hard decision boundaries during training.

<!--
TODO: UPDATE THIS SECTION WITH FINAL RESULTS FROM UPDATED PAPER
## Results

[Insert updated results table and discussion here once paper experiments are finalized.
Key points to cover:
- TMN vs BERT baseline comparison at various parameter budgets
- Updated scaling curve findings
- What the scaling behavior tells us about where the bottleneck is]
-->

## The aggregation bottleneck

Regardless of the specific numbers, one architectural insight has held up across experiments: the bottleneck in this architecture isn't in the propagation step (how nodes exchange information within a tree) but in the aggregation step (how node-level representations get combined into a sentence-level embedding).

The current approach uses mean pooling -- averaging all node representations -- which likely throws away structural information that the propagation step worked hard to encode. If a verb node "knows" about its subject and object through message passing, but then gets averaged together with every other node equally, you lose the hierarchical signal.

My thesis work (in progress) investigates replacing mean pooling with multi-headed self-attention aggregation, essentially using a small transformer as the aggregation function. The hypothesis: more expressive aggregation can unlock the potential of the structural representations by preserving the information that message passing captures, rather than collapsing it into a flat average.

## What this means (and what it doesn't)

**What it means:**
- Explicit structural encoding provides a strong inductive bias, especially at smaller scales. When you don't have billions of parameters to spend on implicitly learning structure, giving the model structure directly helps.
- There's a real question about what transformers learn vs. what could be provided upfront. Parse trees encode genuine linguistic knowledge that otherwise must be extracted from data.
- Resource-efficient approaches can produce meaningful results. Not everything needs a GPU cluster.

**What it doesn't mean:**
- Trees will replace transformers. They won't. The scaling properties of attention are well-demonstrated.
- Structure-based approaches are ready for production. This is research exploring a specific hypothesis about the role of structure in language understanding.

## Where this goes next

The immediate next step is the self-attention aggregation experiments for my thesis. Beyond that, the research direction I find most compelling is what I call the "propagation-then-transformer" approach:

1. Parse each sentence into a dependency tree
2. Run GNN propagation to produce structure-aware node representations
3. Feed those representations (instead of token embeddings) into a transformer

After propagation, the node states are essentially enriched token embeddings -- they carry the same kind of information, plus explicit structural context. The question is whether starting from structurally-informed representations lets you train a smaller or more efficient transformer.

Testing this properly requires compute beyond what's available for a master's thesis. But the TMN results suggest the structural prior has real value, and figuring out how to combine it with the scaling properties of transformers seems like a question worth pursuing.

## Try it yourself

The code is available on [GitHub](https://github.com/jlunder00/Tree-Matching-Networks), and the paper is on [arXiv](https://arxiv.org/abs/2512.00204).

If you have questions or want to discuss tree-based NLP approaches, feel free to reach out -- I'm always happy to talk about this stuff.

---

*Jason Lunder is an ML engineer and researcher at Intellipat Inc. and an MS Computer Science candidate at Eastern Washington University. His research focuses on tree-based architectures for natural language processing.*
